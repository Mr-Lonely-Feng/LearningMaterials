spark工程打包与spark-submit的关系

我们在eclipse编写代码，基于spark api开发自己的大数据计算和处理程序
将我们写好的spark工程打包，比如说java开发环境中，就使用maven assembly插件来打包，将第三方依赖包都打进去
jar包，生产环境中，通常是本机ssh远程连接到部署了spark客户端的linux机器上，使用scp命令将本机的jar包拷贝到远程linux机器上
然后在那个linux机器上，用spark-submit脚本，去将我们的spark工程，作为一次作业/application，提交到集群上去执行

课程中，使用WinSCP工具，从本机windows系统，上传到虚拟机中的linux上去

打包Spark工程

要使用spark-submit提交spark应用程序，首先就必须将我们的spark工程（java/scala）打包成一个jar包
如果我们的spark工程，依赖了其他一些第三方的组件，那就必须把所有组件jar包都打包到工程中，这样才能将完整的工程代码和第三方依赖都分发到spark集群中去
所以必须创建一个assembly jar来包含你所有的代码和依赖，sbt和maven都有assembly插件的（咱们课程里的java工程，就使用了maven的assembly插件）
配置依赖的时候（比如maven工程的pom.xml），可以把Spark和Hadoop的依赖配置成provided类型的依赖，也就是说仅仅开发和编译时有效，打包时就不将这两种依赖打到jar包里去了，因为集群管理器都会提供这些依赖
打好一个assembly jar包之后（也就是你的spark应用程序工程），就可以使用spark-submit脚本提交jar包中的spark应用程序了

spark-submit是什么？

在spark安装目录的bin目录中，有一个spark-submit脚本，这个脚本主要就是用来提交我们自己开发的spark应用程序到集群上执行
spark-submit可以通过一个统一的接口，将spark应用程序提交到所有spark支持的集群管理器上（Standalone（Master）、Yarn（ResourceManager）等）
所以我们并不需要为每种集群管理器都做特殊的配置

--master
1、如果不设置，那么就是local模式
2、如果设置spark://打头的URL，那么就是standalone模式，会提交到指定的URL的Master进程上去
3、如果设置yarn-打头的，那么就是yarn模式，会读取hadoop配置文件，然后连接ResourceManager
