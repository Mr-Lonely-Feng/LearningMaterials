讲师，尽量是实在一点
尽量咱们就来点儿大白话，别一整，很严肃，很死板，有点儿念ppt那个味道
讲原理，轻松一点，随意一点，不要太紧张

spark集群原理概述

搞spark，首先第一点，就是说搭建好大数据集群，spark伪分布式集群（一个节点），master和worker都在一个节点上
你得用spark提供的api，编写大数据计算处理程序，java/scala/python，都可以
程序是针对一份儿，或者多份儿，hdfs、本地文件（很少）、hive、mysql中大数据，至少是千万级别以上的
去根据你的业务需要和需求，去进行计算和处理，最后得到想要的结果

这就是搞这个spark大数据

spark程序写完了以后，就要提交到spark集群上面去运行，这就是spark作业（一次代码的运行+一份数据的处理+一次结果的产出）

spark作业是通过spark集群中的多个独立的进程来并行运行的，每个进程都处理一部分数据，从而做到分布式并行计算，才能对大数据进行处理和计算
作业在多个进程中的运行，是通过SparkContext对象来居中调度的，该对象是在咱们的driver进程中的（包含main方法的程序进程）

SparkContext是支持连接多种集群管理器的（包括Spark Standalone、YARN、Mesos）
集群管理器是负责为SparkContext代表的spark application，在集群中分配资源的
这里说的资源是什么？通俗一点，就是分配多个进程，然后每个进程不都有一些cpu core和内存，有了进程、cpu和内存，你的spark作业才能运行啊

这里说的进程具体是什么呢？怎么工作的呢？

SparkContext会要求集群管理器来分配资源，然后集群管理器就会集群节点上，分配一个或多个executor进程
这些进程就会负责运行你自己写的spark作业代码，每个进程处理一部分数据
具体是怎么运行我们的代码的呢？申请到了executor进程之后，SparkContext会发送我们的工程jar包到executor上，这样，executor就有可以执行的代码了
接着SparkContext会将一些task分发到executor上，每个task执行具体的代码，并处理一小片数据
此外要注意的一点是，executor进程，会会负责存储你的spark作业代码计算出来的一些中间数据，或者是最终结果数据
