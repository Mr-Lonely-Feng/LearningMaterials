除了让flume将数据推送到spark streaming，还有一种方式，可以运行一个自定义的flume sink
1、Flume推送数据到sink中，然后数据缓存在sink中
2、spark streaming用一个可靠的flume receiver以及事务机制从sink中拉取数据

前提条件
1、选择一台可以在flume agent中运行自定义sink的机器
2、将flume的数据管道流配置为将数据传送到那个sink中
3、spark streaming所在的机器可以从那个sink中拉取数据

配置flume

1、加入sink jars，将以下jar加入flume的classpath中

groupId = org.apache.spark
artifactId = spark-streaming-flume-sink_2.10
version = 1.5.1

groupId = org.scala-lang
artifactId = scala-library
version = 2.10.4

groupId = org.apache.commons
artifactId = commons-lang3
version = 3.3.2

2、修改配置文件

agent.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSink
agent.sinks.sink1.hostname = 192.168.0.103
agent.sinks.sink1.port = 8888
agent.sinks.sink1.channel = channel1

配置spark streaming

import org.apache.spark.streaming.flume.*;

JavaReceiverInputDStream<SparkFlumeEvent>flumeStream =
	FlumeUtils.createPollingStream(streamingContext, [sink machine hostname], [sink port]);
	
一定要先启动flume，再启动spark streaming

flume-ng agent -n agent1 -c conf -f /usr/local/flume/conf/flume-conf.properties -Dflume.root.logger=DEBUG,console
